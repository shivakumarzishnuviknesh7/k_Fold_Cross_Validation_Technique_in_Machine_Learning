# -*- coding: utf-8 -*-
"""k_fold_cross_validation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WpHeGWJ5of6kpjDJ5uPE6fNnryDBH3LI

# k-Fold Cross Validation

## Note

## Key Issues Addressed by K-fold Cross-validation:
## Overfitting and Underfitting:

Problem: Overfitting occurs when a model performs well on training data but poorly on unseen data, capturing noise rather than the underlying pattern. Underfitting happens when a model is too simple to capture the underlying trend in the data.
Solution: By splitting the data into k subsets and training/testing the model k times, each time with a different subset as the test set and the remaining as the training set, k-fold cross-validation ensures the model is evaluated on different segments of the data. This helps in assessing how well the model generalizes to new data.

## Bias-Variance Trade-off:
Problem: A model with high bias pays little attention to the training data and may underfit, while a model with high variance pays too much attention to the training data and may overfit.
Solution: K-fold cross-validation helps in finding a balance between bias and variance by providing a more reliable estimate of model performance. It reduces the likelihood of relying on a single train-test split, which might be biased or unrepresentative of the whole dataset.

## Data Utilization:
Problem: In simple train-test splits, a significant portion of data is reserved for testing, which means less data is available for training the model.
Solution: K-fold cross-validation allows the model to be trained and tested multiple times on different data subsets, ensuring that every data point is used for both training and validation. This leads to more efficient use of the available data.

## Performance Estimation:
Problem: Single train-test splits can result in performance estimates that are highly dependent on the particular split used, which may not be representative of the model’s performance on unseen data.
Solution: By averaging the results of the k different runs, k-fold cross-validation provides a more robust and reliable estimate of the model’s performance.

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Training the Kernel SVM model on the Training set"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""## Applying k-Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)#cv = 10 -  we create 10 fold so we get 10 accuracy #we apply kfold on training set X = X_train, y = y_train
print(accuracies)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100)) # getting average of all vectors
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100)) # getting standard deviation

"""## Visualising the Training set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""## Visualising the Test set results"""

from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()